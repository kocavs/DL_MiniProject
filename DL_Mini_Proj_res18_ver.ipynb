{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet For CIFAR0-10"
      ],
      "metadata": {
        "id": "5Cgn58bGLA7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/My\\ Drive/DL_Mini_Proj\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzleoGvIa3JV",
        "outputId": "91e77ee7-d562-459b-8e93-07e053c1e3ea"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "[Errno 2] No such file or directory: '/content/drive/My Drive/DL_Mini_Proj'\n",
            "/content\n",
            "drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "#import renet\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "from torchsummary import summary\n",
        "import torch.utils.data as data"
      ],
      "metadata": {
        "id": "enCe4Ish2n47"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Construction"
      ],
      "metadata": {
        "id": "EgqUa4LeK3Tr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes) # ochange\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])"
      ],
      "metadata": {
        "id": "13PaS06K0N3g"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = ResNet18().to(device)\n",
        "model.layer4 = torch.nn.Sequential(*[model.layer4[0]]) # https://discuss.pytorch.org/t/how-to-delete-layer-in-pretrained-model/17648/4\n",
        "model.layer3 = torch.nn.Sequential(*[model.layer3[0]])\n",
        "model.layer1 = torch.nn.Sequential(*[model.layer1[0]])\n",
        "model.layer2 = torch.nn.Sequential(*[model.layer2[0]])"
      ],
      "metadata": {
        "id": "JZ5l09322n6z"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model, (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f99yZ7-kNwN9",
        "outputId": "a4b5755c-0eec-46af-a705-4c8c7c975590"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
            "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
            "            Conv2d-3           [-1, 64, 32, 32]          36,864\n",
            "       BatchNorm2d-4           [-1, 64, 32, 32]             128\n",
            "            Conv2d-5           [-1, 64, 32, 32]          36,864\n",
            "       BatchNorm2d-6           [-1, 64, 32, 32]             128\n",
            "        BasicBlock-7           [-1, 64, 32, 32]               0\n",
            "            Conv2d-8          [-1, 128, 16, 16]          73,728\n",
            "       BatchNorm2d-9          [-1, 128, 16, 16]             256\n",
            "           Conv2d-10          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-11          [-1, 128, 16, 16]             256\n",
            "           Conv2d-12          [-1, 128, 16, 16]           8,192\n",
            "      BatchNorm2d-13          [-1, 128, 16, 16]             256\n",
            "       BasicBlock-14          [-1, 128, 16, 16]               0\n",
            "           Conv2d-15            [-1, 256, 8, 8]         294,912\n",
            "      BatchNorm2d-16            [-1, 256, 8, 8]             512\n",
            "           Conv2d-17            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-18            [-1, 256, 8, 8]             512\n",
            "           Conv2d-19            [-1, 256, 8, 8]          32,768\n",
            "      BatchNorm2d-20            [-1, 256, 8, 8]             512\n",
            "       BasicBlock-21            [-1, 256, 8, 8]               0\n",
            "           Conv2d-22            [-1, 512, 4, 4]       1,179,648\n",
            "      BatchNorm2d-23            [-1, 512, 4, 4]           1,024\n",
            "           Conv2d-24            [-1, 512, 4, 4]       2,359,296\n",
            "      BatchNorm2d-25            [-1, 512, 4, 4]           1,024\n",
            "           Conv2d-26            [-1, 512, 4, 4]         131,072\n",
            "      BatchNorm2d-27            [-1, 512, 4, 4]           1,024\n",
            "       BasicBlock-28            [-1, 512, 4, 4]               0\n",
            "           Linear-29                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 4,903,242\n",
            "Trainable params: 4,903,242\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 6.56\n",
            "Params size (MB): 18.70\n",
            "Estimated Total Size (MB): 25.28\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construction Complete"
      ],
      "metadata": {
        "id": "P1F2LXoFL5K8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing"
      ],
      "metadata": {
        "id": "QwDtf6ypL-E9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Cutout(object):\n",
        "  \"\"\"Randomly mask out one or more patches from an image.\n",
        "  Args:\n",
        "      n_holes (int): Number of patches to cut out of each image.\n",
        "      length (int): The length (in pixels) of each square patch.\n",
        "  \"\"\"\n",
        "  def __init__(self, n_holes, length):\n",
        "      self.n_holes = n_holes\n",
        "      self.length = length\n",
        "\n",
        "  def __call__(self, img):\n",
        "      \"\"\"\n",
        "      Args:\n",
        "          img (Tensor): Tensor image of size (C, H, W).\n",
        "      Returns:\n",
        "          Tensor: Image with n_holes of dimension length x length cut out of it.\n",
        "      \"\"\"\n",
        "      h = img.size(1)\n",
        "      w = img.size(2)\n",
        "\n",
        "      mask = np.ones((h, w), np.float32)\n",
        "\n",
        "      for n in range(self.n_holes):\n",
        "          y = np.random.randint(h)\n",
        "          x = np.random.randint(w)\n",
        "\n",
        "          y1 = np.clip(y - self.length // 2, 0, h)\n",
        "          y2 = np.clip(y + self.length // 2, 0, h)\n",
        "          x1 = np.clip(x - self.length // 2, 0, w)\n",
        "          x2 = np.clip(x + self.length // 2, 0, w)\n",
        "\n",
        "          mask[y1: y2, x1: x2] = 0.\n",
        "\n",
        "      mask = torch.from_numpy(mask)\n",
        "      mask = mask.expand_as(img)\n",
        "      img = img * mask\n",
        "\n",
        "      return img"
      ],
      "metadata": {
        "id": "TneJZzB8Jmqs"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_CIFAR10(batch_size, train_ratio):\n",
        "\n",
        "  ROOT = './data'\n",
        "  trainset = torchvision.datasets.CIFAR10(\n",
        "      root = ROOT,\n",
        "      train = True, \n",
        "      download = True\n",
        "  )\n",
        "\n",
        "  # Compute means and standard deviations\n",
        "  means = trainset.data.mean(axis=(0,1,2)) / 255\n",
        "  stds = trainset.data.std(axis=(0,1,2)) / 255\n",
        "  #print(means, stds)\n",
        "\n",
        "  # Preprocess setting\n",
        "  transform_train = transforms.Compose([\n",
        "      transforms.RandomCrop(32, padding=4),\n",
        "      transforms.RandomHorizontalFlip(),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean=means, std=stds),\n",
        "      Cutout(n_holes=1, length=16)\n",
        "  ])\n",
        "  transform_test = transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean=means, std=stds)\n",
        "  ])\n",
        "\n",
        "  # Load the dataset\n",
        "  trainset = torchvision.datasets.CIFAR10(\n",
        "      root = ROOT, \n",
        "      train = True, \n",
        "      download = True, \n",
        "      transform = transform_train\n",
        "  )\n",
        "  testset = torchvision.datasets.CIFAR10(\n",
        "      root = ROOT, \n",
        "      train = False, \n",
        "      download = True, \n",
        "      transform = transform_test\n",
        "  )\n",
        "\n",
        "  train_iterator = data.DataLoader(trainset, batch_size)\n",
        "  test_iterator = data.DataLoader(testset, batch_size)\n",
        "\n",
        "  return train_iterator, test_iterator\n",
        "  \"\"\"\n",
        "  # Split trainset for validset\n",
        "  n_train = int(len(trainset) * train_ratio)\n",
        "  n_valid = len(trainset) - n_train\n",
        "  train_dataset, valid_dataset = data.random_split(trainset, [n_train, n_valid])\n",
        "  \n",
        "  # Build dataloader\n",
        "  train_iterator = data.DataLoader(train_dataset, batch_size)\n",
        "  valid_iterator = data.DataLoader(valid_dataset, batch_size)\n",
        "  test_iterator = data.DataLoader(testset, batch_size)\n",
        "\n",
        "  return train_iterator, valid_iterator, test_iterator\n",
        "  \"\"\""
      ],
      "metadata": {
        "id": "1DTMZC3WJmwF"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainloader, validloader, testloader = load_CIFAR10(batch_size=16, train_ratio=1)\n",
        "trainloader, testloader = load_CIFAR10(batch_size=16, train_ratio=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21PibWhZJ6da",
        "outputId": "f0e9543f-54b8-4e1f-bbcc-25d6592b3acf"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:01<00:00, 102247498.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "YcJteBmC2n8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "0289f7d2-082b-42e9-b666-b5a130764090"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntransform_train = transforms.Compose([\\n    transforms.RandomCrop(32, padding=4),\\n    transforms.RandomHorizontalFlip(),\\n    transforms.ToTensor(),\\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\\n])\\n\\ntransform_test = transforms.Compose([\\n    transforms.ToTensor(),\\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\\n])\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_train) # change transform in future\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test) # change transform in future\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4HFVdimn2n-O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "9fded594-637f-4fa0-a53c-6f620dd33ff6"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ntrainset = torchvision.datasets.CIFAR10(\\n    root='./data', train=True, download=True, transform=transform_train) # change transform in future\\ntrainloader = torch.utils.data.DataLoader(\\n    trainset, batch_size=128, shuffle=True, num_workers=2)\\n\\ntestset = torchvision.datasets.CIFAR10(\\n    root='./data', train=False, download=True, transform=transform_test) # change transform in future\\ntestloader = torch.utils.data.DataLoader(\\n    testset, batch_size=100, shuffle=False, num_workers=2)\\n\\nclasses = ('plane', 'car', 'bird', 'cat', 'deer',\\n           'dog', 'frog', 'horse', 'ship', 'truck')\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)#,weight_decay=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
      ],
      "metadata": {
        "id": "v6RvRO3e6Rjp"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "    print(\"Train Set Loss:\",train_loss/total)\n",
        "\n"
      ],
      "metadata": {
        "id": "JbBSkXWE2n_7"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(epoch):\n",
        "    global best_acc\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    # Save checkpoint.\n",
        "    acc = 100.*correct/total\n",
        "    if acc > best_acc:\n",
        "        print('Saving..')\n",
        "        state = {\n",
        "            'net': model.state_dict(),\n",
        "            'acc': acc,\n",
        "            'epoch': epoch,\n",
        "        }\n",
        "        if not os.path.isdir('checkpoint'):\n",
        "            os.mkdir('checkpoint')\n",
        "        torch.save(state, './checkpoint/ckpt.pth')\n",
        "        best_acc = acc\n",
        "    print('Test Set Accuracy:',acc)\n",
        "    return acc\n"
      ],
      "metadata": {
        "id": "LxH7SkJJ2oBp"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
        "\n",
        "for epoch in range(start_epoch, start_epoch+20):\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "id": "pzs360FC2oKB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bdfa910-472b-476a-c926-4adb078c2112"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 0\n",
            "Train Set Loss: 0.11250551059007645\n",
            "Saving..\n",
            "Test Set Accuracy: 45.42\n",
            "\n",
            "Epoch: 1\n",
            "Train Set Loss: 0.08646546324849129\n",
            "Saving..\n",
            "Test Set Accuracy: 56.74\n",
            "\n",
            "Epoch: 2\n",
            "Train Set Loss: 0.07316700073242187\n",
            "Saving..\n",
            "Test Set Accuracy: 65.77\n",
            "\n",
            "Epoch: 3\n",
            "Train Set Loss: 0.06538874449670315\n",
            "Saving..\n",
            "Test Set Accuracy: 70.77\n",
            "\n",
            "Epoch: 4\n",
            "Train Set Loss: 0.059088009424209595\n",
            "Saving..\n",
            "Test Set Accuracy: 73.96\n",
            "\n",
            "Epoch: 5\n",
            "Train Set Loss: 0.053653905138671396\n",
            "Saving..\n",
            "Test Set Accuracy: 75.2\n",
            "\n",
            "Epoch: 6\n",
            "Train Set Loss: 0.04969362676978111\n",
            "Saving..\n",
            "Test Set Accuracy: 77.02\n",
            "\n",
            "Epoch: 7\n",
            "Train Set Loss: 0.04623111477643251\n",
            "Saving..\n",
            "Test Set Accuracy: 80.83\n",
            "\n",
            "Epoch: 8\n",
            "Train Set Loss: 0.043636488420963286\n",
            "Saving..\n",
            "Test Set Accuracy: 81.01\n",
            "\n",
            "Epoch: 9\n",
            "Train Set Loss: 0.04136169065803289\n",
            "Saving..\n",
            "Test Set Accuracy: 81.65\n",
            "\n",
            "Epoch: 10\n",
            "Train Set Loss: 0.039682828458100554\n",
            "Saving..\n",
            "Test Set Accuracy: 82.05\n",
            "\n",
            "Epoch: 11\n",
            "Train Set Loss: 0.037984485948830846\n",
            "Saving..\n",
            "Test Set Accuracy: 83.23\n",
            "\n",
            "Epoch: 12\n",
            "Train Set Loss: 0.036559242131859064\n",
            "Saving..\n",
            "Test Set Accuracy: 84.56\n",
            "\n",
            "Epoch: 13\n",
            "Train Set Loss: 0.035210863521844146\n",
            "Saving..\n",
            "Test Set Accuracy: 84.84\n",
            "\n",
            "Epoch: 14\n",
            "Train Set Loss: 0.03415071791931987\n",
            "Saving..\n",
            "Test Set Accuracy: 84.93\n",
            "\n",
            "Epoch: 15\n",
            "Train Set Loss: 0.03309123551860452\n",
            "Saving..\n",
            "Test Set Accuracy: 85.56\n",
            "\n",
            "Epoch: 16\n",
            "Train Set Loss: 0.031980079222917554\n",
            "Saving..\n",
            "Test Set Accuracy: 86.37\n",
            "\n",
            "Epoch: 17\n",
            "Train Set Loss: 0.030868731017261744\n",
            "Test Set Accuracy: 85.83\n",
            "\n",
            "Epoch: 18\n",
            "Train Set Loss: 0.03017510926246643\n",
            "Test Set Accuracy: 86.17\n",
            "\n",
            "Epoch: 19\n",
            "Train Set Loss: 0.02876965137053281\n",
            "Saving..\n",
            "Test Set Accuracy: 86.74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for g in optimizer.param_groups:\n",
        "    print(optimizer.param_groups[0]['lr'])\n",
        "    g['lr'] = 0.001\n",
        "\"\"\"\n",
        "state = {\n",
        "      'net': model.state_dict()\n",
        "}\n",
        "if not os.path.isdir('checkpoint'):\n",
        "  os.mkdir('checkpoint')\n",
        "torch.save(state, './checkpoint/20Epoch.pth')"
      ],
      "metadata": {
        "id": "1XS9hEnaR0M2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001,weight_decay=1e-6)"
      ],
      "metadata": {
        "id": "43wfxq1E1Xy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 20  \n",
        "\n",
        "for epoch in range(start_epoch, start_epoch+20):\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulxHcGSzRFwn",
        "outputId": "825c43ff-1784-4153-819e-ccd7744f93e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 20\n",
            "Train Set Loss: 0.06111984935224056\n",
            "Test Set Accuracy: 76.53\n",
            "\n",
            "Epoch: 21\n",
            "Train Set Loss: 0.056872786796987054\n",
            "Test Set Accuracy: 77.37\n",
            "\n",
            "Epoch: 22\n",
            "Train Set Loss: 0.054467707093656066\n",
            "Test Set Accuracy: 78.47\n",
            "\n",
            "Epoch: 23\n",
            "Train Set Loss: 0.052736565395891666\n",
            "Test Set Accuracy: 79.78\n",
            "\n",
            "Epoch: 24\n",
            "Train Set Loss: 0.05089716591179371\n",
            "Saving..\n",
            "Test Set Accuracy: 79.98\n",
            "\n",
            "Epoch: 25\n",
            "Train Set Loss: 0.04973517975986004\n",
            "Saving..\n",
            "Test Set Accuracy: 80.31\n",
            "\n",
            "Epoch: 26\n",
            "Train Set Loss: 0.048367037560641764\n",
            "Saving..\n",
            "Test Set Accuracy: 80.35\n",
            "\n",
            "Epoch: 27\n",
            "Train Set Loss: 0.047496571877896784\n",
            "Saving..\n",
            "Test Set Accuracy: 81.09\n",
            "\n",
            "Epoch: 28\n",
            "Train Set Loss: 0.04673378049641848\n",
            "Saving..\n",
            "Test Set Accuracy: 81.87\n",
            "\n",
            "Epoch: 29\n",
            "Train Set Loss: 0.04588644198894501\n",
            "Test Set Accuracy: 81.37\n",
            "\n",
            "Epoch: 30\n",
            "Train Set Loss: 0.04482184986442327\n",
            "Test Set Accuracy: 81.79\n",
            "\n",
            "Epoch: 31\n",
            "Train Set Loss: 0.04400566786378622\n",
            "Saving..\n",
            "Test Set Accuracy: 83.04\n",
            "\n",
            "Epoch: 32\n",
            "Train Set Loss: 0.04339637280225754\n",
            "Test Set Accuracy: 83.0\n",
            "\n",
            "Epoch: 33\n",
            "Train Set Loss: 0.04295982111155987\n",
            "Test Set Accuracy: 82.9\n",
            "\n",
            "Epoch: 34\n",
            "Train Set Loss: 0.04221132900506258\n",
            "Saving..\n",
            "Test Set Accuracy: 83.48\n",
            "\n",
            "Epoch: 35\n",
            "Train Set Loss: 0.04187333113223314\n",
            "Saving..\n",
            "Test Set Accuracy: 83.66\n",
            "\n",
            "Epoch: 36\n",
            "Train Set Loss: 0.040972914422899485\n",
            "Test Set Accuracy: 83.16\n",
            "\n",
            "Epoch: 37\n",
            "Train Set Loss: 0.04076682301700115\n",
            "Saving..\n",
            "Test Set Accuracy: 83.74\n",
            "\n",
            "Epoch: 38\n",
            "Train Set Loss: 0.03991217172145844\n",
            "Saving..\n",
            "Test Set Accuracy: 84.29\n",
            "\n",
            "Epoch: 39\n",
            "Train Set Loss: 0.039652029202282425\n",
            "Saving..\n",
            "Test Set Accuracy: 84.73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 40  \n",
        "\n",
        "for epoch in range(start_epoch, start_epoch+20):\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "id": "9AFmB7xGRF6S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84197420-704a-4a33-ec07-119a2cc6e6bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 40\n",
            "Train Set Loss: 0.03933024534359574\n",
            "Test Set Accuracy: 84.0\n",
            "\n",
            "Epoch: 41\n",
            "Train Set Loss: 0.03863885339826346\n",
            "Test Set Accuracy: 83.97\n",
            "\n",
            "Epoch: 42\n",
            "Train Set Loss: 0.038449449110776183\n",
            "Saving..\n",
            "Test Set Accuracy: 84.91\n",
            "\n",
            "Epoch: 43\n",
            "Train Set Loss: 0.03798370545908809\n",
            "Test Set Accuracy: 84.66\n",
            "\n",
            "Epoch: 44\n",
            "Train Set Loss: 0.037390205317288636\n",
            "Test Set Accuracy: 84.39\n",
            "\n",
            "Epoch: 45\n",
            "Train Set Loss: 0.036924358177632094\n",
            "Test Set Accuracy: 84.75\n",
            "\n",
            "Epoch: 46\n",
            "Train Set Loss: 0.036688906094133854\n",
            "Test Set Accuracy: 84.78\n",
            "\n",
            "Epoch: 47\n",
            "Train Set Loss: 0.03652169727616012\n",
            "Saving..\n",
            "Test Set Accuracy: 85.04\n",
            "\n",
            "Epoch: 48\n",
            "Train Set Loss: 0.03618277742251754\n",
            "Saving..\n",
            "Test Set Accuracy: 85.47\n",
            "\n",
            "Epoch: 49\n",
            "Train Set Loss: 0.03565346192106605\n",
            "Test Set Accuracy: 85.18\n",
            "\n",
            "Epoch: 50\n",
            "Train Set Loss: 0.035647349169552325\n",
            "Test Set Accuracy: 85.31\n",
            "\n",
            "Epoch: 51\n",
            "Train Set Loss: 0.03535126780733466\n",
            "Test Set Accuracy: 84.88\n",
            "\n",
            "Epoch: 52\n",
            "Train Set Loss: 0.03477174524277449\n",
            "Saving..\n",
            "Test Set Accuracy: 85.71\n",
            "\n",
            "Epoch: 53\n",
            "Train Set Loss: 0.03441815001085401\n",
            "Test Set Accuracy: 85.27\n",
            "\n",
            "Epoch: 54\n",
            "Train Set Loss: 0.03461835622176528\n",
            "Test Set Accuracy: 85.63\n",
            "\n",
            "Epoch: 55\n",
            "Train Set Loss: 0.03403878188207746\n",
            "Saving..\n",
            "Test Set Accuracy: 85.91\n",
            "\n",
            "Epoch: 56\n",
            "Train Set Loss: 0.03389089255958796\n",
            "Test Set Accuracy: 85.64\n",
            "\n",
            "Epoch: 57\n",
            "Train Set Loss: 0.03371512546852231\n",
            "Test Set Accuracy: 85.84\n",
            "\n",
            "Epoch: 58\n",
            "Train Set Loss: 0.033417165852412584\n",
            "Saving..\n",
            "Test Set Accuracy: 86.12\n",
            "\n",
            "Epoch: 59\n",
            "Train Set Loss: 0.032972260534390806\n",
            "Test Set Accuracy: 86.06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zoRGtBBaRF8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q3OJaGSsRGCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8m0tUNP3QvhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Accuracy: ',test(0))"
      ],
      "metadata": {
        "id": "COW3YCVV2oNa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a31f094-4823-4971-f7df-64c6d6dc6deb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving..\n",
            "Accuracy: 10.0\n",
            "Accuracy:  10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VkQa44cg2oUo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}