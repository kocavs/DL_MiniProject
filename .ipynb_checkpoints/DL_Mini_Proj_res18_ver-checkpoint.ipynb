{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Cgn58bGLA7l"
   },
   "source": [
    "# ResNet For CIFAR0-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EzleoGvIa3JV",
    "outputId": "91e77ee7-d562-459b-8e93-07e053c1e3ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\\n\\n%cd /content/drive/My\\\\ Drive/DL_Mini_Proj\\n!ls\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "%cd /content/drive/My\\ Drive/DL_Mini_Proj\n",
    "!ls\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "enCe4Ish2n47"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "#import renet\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "from torchsummary import summary\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EgqUa4LeK3Tr"
   },
   "source": [
    "### Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "13PaS06K0N3g"
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes) # ochange\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JZ5l09322n6z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): BasicBlock(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential()\n",
      "  )\n",
      "  (1): BasicBlock(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential()\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (0): BasicBlock(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = ResNet18().to(device)\n",
    "print(model.layer1)\n",
    "model.layer4 = torch.nn.Sequential(*[model.layer4[0]]) # https://discuss.pytorch.org/t/how-to-delete-layer-in-pretrained-model/17648/4\n",
    "model.layer3 = torch.nn.Sequential(*[model.layer3[0]])\n",
    "model.layer1 = torch.nn.Sequential(*[model.layer1[0]])\n",
    "model.layer2 = torch.nn.Sequential(*[model.layer2[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f99yZ7-kNwN9",
    "outputId": "a4b5755c-0eec-46af-a705-4c8c7c975590"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "            Conv2d-3           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-4           [-1, 64, 32, 32]             128\n",
      "            Conv2d-5           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 32, 32]             128\n",
      "        BasicBlock-7           [-1, 64, 32, 32]               0\n",
      "            Conv2d-8          [-1, 128, 16, 16]          73,728\n",
      "       BatchNorm2d-9          [-1, 128, 16, 16]             256\n",
      "           Conv2d-10          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-11          [-1, 128, 16, 16]             256\n",
      "           Conv2d-12          [-1, 128, 16, 16]           8,192\n",
      "      BatchNorm2d-13          [-1, 128, 16, 16]             256\n",
      "       BasicBlock-14          [-1, 128, 16, 16]               0\n",
      "           Conv2d-15            [-1, 256, 8, 8]         294,912\n",
      "      BatchNorm2d-16            [-1, 256, 8, 8]             512\n",
      "           Conv2d-17            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-18            [-1, 256, 8, 8]             512\n",
      "           Conv2d-19            [-1, 256, 8, 8]          32,768\n",
      "      BatchNorm2d-20            [-1, 256, 8, 8]             512\n",
      "       BasicBlock-21            [-1, 256, 8, 8]               0\n",
      "           Conv2d-22            [-1, 512, 4, 4]       1,179,648\n",
      "      BatchNorm2d-23            [-1, 512, 4, 4]           1,024\n",
      "           Conv2d-24            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-25            [-1, 512, 4, 4]           1,024\n",
      "           Conv2d-26            [-1, 512, 4, 4]         131,072\n",
      "      BatchNorm2d-27            [-1, 512, 4, 4]           1,024\n",
      "       BasicBlock-28            [-1, 512, 4, 4]               0\n",
      "           Linear-29                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 4,903,242\n",
      "Trainable params: 4,903,242\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 6.56\n",
      "Params size (MB): 18.70\n",
      "Estimated Total Size (MB): 25.28\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1F2LXoFL5K8"
   },
   "source": [
    "Construction Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwDtf6ypL-E9"
   },
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TneJZzB8Jmqs"
   },
   "outputs": [],
   "source": [
    "class Cutout(object):\n",
    "    \"\"\"Randomly mask out one or more patches from an image.\n",
    "    Args:\n",
    "    n_holes (int): Number of patches to cut out of each image.\n",
    "    length (int): The length (in pixels) of each square patch.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_holes, length):\n",
    "      self.n_holes = n_holes\n",
    "      self.length = length\n",
    "\n",
    "    def __call__(self, img):\n",
    "        h = img.size(1)\n",
    "        w = img.size(2)\n",
    "\n",
    "        mask = np.ones((h, w), np.float32)\n",
    "    \n",
    "        for n in range(self.n_holes):\n",
    "            y = np.random.randint(h)\n",
    "            x = np.random.randint(w)\n",
    "    \n",
    "            y1 = np.clip(y - self.length // 2, 0, h)\n",
    "            y2 = np.clip(y + self.length // 2, 0, h)\n",
    "            x1 = np.clip(x - self.length // 2, 0, w)\n",
    "            x2 = np.clip(x + self.length // 2, 0, w)\n",
    "\n",
    "            mask[y1: y2, x1: x2] = 0.\n",
    "    \n",
    "\n",
    "        mask = torch.from_numpy(mask)\n",
    "          \n",
    "        mask = mask.expand_as(img)\n",
    "              \n",
    "        img = img * mask\n",
    "\n",
    "    \n",
    "      \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1DTMZC3WJmwF"
   },
   "outputs": [],
   "source": [
    "def load_CIFAR10(batch_size, train_ratio):\n",
    "\n",
    "  ROOT = '/scratch/hx2214/data'\n",
    "  trainset = torchvision.datasets.CIFAR10(\n",
    "      root = ROOT,\n",
    "      train = True, \n",
    "      download = True\n",
    "  )\n",
    "\n",
    "  # Compute means and standard deviations\n",
    "  means = trainset.data.mean(axis=(0,1,2)) / 255\n",
    "  stds = trainset.data.std(axis=(0,1,2)) / 255\n",
    "  #print(means, stds)\n",
    "\n",
    "  # Preprocess setting\n",
    "  transform_train = transforms.Compose([\n",
    "      transforms.RandomCrop(32, padding=4),\n",
    "      transforms.RandomHorizontalFlip(),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(mean=means, std=stds),\n",
    "      Cutout(n_holes=1, length=16)\n",
    "  ])\n",
    "  transform_test = transforms.Compose([\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(mean=means, std=stds)\n",
    "  ])\n",
    "\n",
    "  # Load the dataset\n",
    "  trainset = torchvision.datasets.CIFAR10(\n",
    "      root = ROOT, \n",
    "      train = True, \n",
    "      download = True, \n",
    "      transform = transform_train\n",
    "  )\n",
    "  testset = torchvision.datasets.CIFAR10(\n",
    "      root = ROOT, \n",
    "      train = False, \n",
    "      download = True, \n",
    "      transform = transform_test\n",
    "  )\n",
    "\n",
    "  train_iterator = data.DataLoader(trainset, batch_size)\n",
    "  test_iterator = data.DataLoader(testset, batch_size)\n",
    "\n",
    "  return train_iterator, test_iterator\n",
    "  \"\"\"\n",
    "  # Split trainset for validset\n",
    "  n_train = int(len(trainset) * train_ratio)\n",
    "  n_valid = len(trainset) - n_train\n",
    "  train_dataset, valid_dataset = data.random_split(trainset, [n_train, n_valid])\n",
    "  \n",
    "  # Build dataloader\n",
    "  train_iterator = data.DataLoader(train_dataset, batch_size)\n",
    "  valid_iterator = data.DataLoader(valid_dataset, batch_size)\n",
    "  test_iterator = data.DataLoader(testset, batch_size)\n",
    "\n",
    "  return train_iterator, valid_iterator, test_iterator\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21PibWhZJ6da",
    "outputId": "f0e9543f-54b8-4e1f-bbcc-25d6592b3acf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# trainloader, validloader, testloader = load_CIFAR10(batch_size=16, train_ratio=1)\n",
    "trainloader, testloader = load_CIFAR10(batch_size=16, train_ratio=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "YcJteBmC2n8b",
    "outputId": "0289f7d2-082b-42e9-b666-b5a130764090"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\ntransform_train = transforms.Compose([\\n    transforms.RandomCrop(32, padding=4),\\n    transforms.RandomHorizontalFlip(),\\n    transforms.ToTensor(),\\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\\n])\\n\\ntransform_test = transforms.Compose([\\n    transforms.ToTensor(),\\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\\n])\\n'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "4HFVdimn2n-O",
    "outputId": "9fded594-637f-4fa0-a53c-6f620dd33ff6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"\\ntrainset = torchvision.datasets.CIFAR10(\\n    root='./data', train=True, download=True, transform=transform_train) # change transform in future\\ntrainloader = torch.utils.data.DataLoader(\\n    trainset, batch_size=128, shuffle=True, num_workers=2)\\n\\ntestset = torchvision.datasets.CIFAR10(\\n    root='./data', train=False, download=True, transform=transform_test) # change transform in future\\ntestloader = torch.utils.data.DataLoader(\\n    testset, batch_size=100, shuffle=False, num_workers=2)\\n\\nclasses = ('plane', 'car', 'bird', 'cat', 'deer',\\n           'dog', 'frog', 'horse', 'ship', 'truck')\\n\""
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train) # change transform in future\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test) # change transform in future\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "v6RvRO3e6Rjp"
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)#,weight_decay=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "JbBSkXWE2n_7"
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    print(\"Train Set Loss:\",train_loss/total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "LxH7SkJJ2oBp"
   },
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    global best_acc\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': model.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/ckpt.pth')\n",
    "        best_acc = acc\n",
    "    print('Test Set Accuracy:',acc)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pzs360FC2oKB",
    "outputId": "6bdfa910-472b-476a-c926-4adb078c2112"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Train Set Loss: 0.11087040110230446\n",
      "Saving..\n",
      "Test Set Accuracy: 45.15\n",
      "\n",
      "Epoch: 1\n",
      "Train Set Loss: 0.0861809755575657\n",
      "Saving..\n",
      "Test Set Accuracy: 59.9\n",
      "\n",
      "Epoch: 2\n",
      "Train Set Loss: 0.07349206447184085\n",
      "Saving..\n",
      "Test Set Accuracy: 62.27\n",
      "\n",
      "Epoch: 3\n",
      "Train Set Loss: 0.06549771600067615\n",
      "Saving..\n",
      "Test Set Accuracy: 70.29\n",
      "\n",
      "Epoch: 4\n",
      "Train Set Loss: 0.05974413369119167\n",
      "Saving..\n",
      "Test Set Accuracy: 72.2\n",
      "\n",
      "Epoch: 5\n",
      "Train Set Loss: 0.05427168503075838\n",
      "Saving..\n",
      "Test Set Accuracy: 75.34\n",
      "\n",
      "Epoch: 6\n",
      "Train Set Loss: 0.04951760059475899\n",
      "Saving..\n",
      "Test Set Accuracy: 77.61\n",
      "\n",
      "Epoch: 7\n",
      "Train Set Loss: 0.0462010094127059\n",
      "Saving..\n",
      "Test Set Accuracy: 78.74\n",
      "\n",
      "Epoch: 8\n",
      "Train Set Loss: 0.04342289152622223\n",
      "Saving..\n",
      "Test Set Accuracy: 81.82\n",
      "\n",
      "Epoch: 9\n",
      "Train Set Loss: 0.04146881461083889\n",
      "Saving..\n",
      "Test Set Accuracy: 82.07\n",
      "\n",
      "Epoch: 10\n",
      "Train Set Loss: 0.03902833487033844\n",
      "Test Set Accuracy: 81.9\n",
      "\n",
      "Epoch: 11\n",
      "Train Set Loss: 0.03778805713400245\n",
      "Saving..\n",
      "Test Set Accuracy: 82.54\n",
      "\n",
      "Epoch: 12\n",
      "Train Set Loss: 0.03632956011362374\n",
      "Saving..\n",
      "Test Set Accuracy: 85.3\n",
      "\n",
      "Epoch: 13\n",
      "Train Set Loss: 0.03453959132581949\n",
      "Test Set Accuracy: 83.09\n",
      "\n",
      "Epoch: 14\n",
      "Train Set Loss: 0.033836713696196674\n",
      "Saving..\n",
      "Test Set Accuracy: 85.48\n",
      "\n",
      "Epoch: 15\n",
      "Train Set Loss: 0.032627474329918624\n",
      "Saving..\n",
      "Test Set Accuracy: 85.71\n",
      "\n",
      "Epoch: 16\n",
      "Train Set Loss: 0.03151038385465741\n",
      "Test Set Accuracy: 85.42\n",
      "\n",
      "Epoch: 17\n",
      "Train Set Loss: 0.030381820162385702\n",
      "Saving..\n",
      "Test Set Accuracy: 85.97\n",
      "\n",
      "Epoch: 18\n",
      "Train Set Loss: 0.029599150112569333\n",
      "Saving..\n",
      "Test Set Accuracy: 86.88\n",
      "\n",
      "Epoch: 19\n",
      "Train Set Loss: 0.029179007461741565\n",
      "Test Set Accuracy: 86.57\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch+20):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "1XS9hEnaR0M2"
   },
   "outputs": [],
   "source": [
    "state = {\n",
    "      'net': model.state_dict()\n",
    "}\n",
    "if not os.path.isdir('checkpoint'):\n",
    "  os.mkdir('checkpoint')\n",
    "torch.save(state, './checkpoint/20Epoch.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "43wfxq1E1Xy9"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001,weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ulxHcGSzRFwn",
    "outputId": "825c43ff-1784-4153-819e-ccd7744f93e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 20\n",
      "Train Set Loss: 0.023478635339643807\n",
      "Saving..\n",
      "Test Set Accuracy: 89.11\n",
      "\n",
      "Epoch: 21\n",
      "Train Set Loss: 0.0223777623976022\n",
      "Saving..\n",
      "Test Set Accuracy: 89.16\n",
      "\n",
      "Epoch: 22\n",
      "Train Set Loss: 0.02129191546998918\n",
      "Saving..\n",
      "Test Set Accuracy: 89.44\n",
      "\n",
      "Epoch: 23\n",
      "Train Set Loss: 0.021413901176527143\n",
      "Test Set Accuracy: 89.31\n",
      "\n",
      "Epoch: 24\n",
      "Train Set Loss: 0.02070578764460981\n",
      "Saving..\n",
      "Test Set Accuracy: 89.63\n",
      "\n",
      "Epoch: 25\n",
      "Train Set Loss: 0.02046374794024974\n",
      "Saving..\n",
      "Test Set Accuracy: 89.7\n",
      "\n",
      "Epoch: 26\n",
      "Train Set Loss: 0.02014391047986224\n",
      "Saving..\n",
      "Test Set Accuracy: 89.87\n",
      "\n",
      "Epoch: 27\n",
      "Train Set Loss: 0.01987136450753547\n",
      "Test Set Accuracy: 89.82\n",
      "\n",
      "Epoch: 28\n",
      "Train Set Loss: 0.01971722053712234\n",
      "Saving..\n",
      "Test Set Accuracy: 89.97\n",
      "\n",
      "Epoch: 29\n",
      "Train Set Loss: 0.019586234425231816\n",
      "Test Set Accuracy: 89.9\n",
      "\n",
      "Epoch: 30\n",
      "Train Set Loss: 0.0190031138999667\n",
      "Saving..\n",
      "Test Set Accuracy: 90.08\n",
      "\n",
      "Epoch: 31\n",
      "Train Set Loss: 0.018994125402458012\n",
      "Test Set Accuracy: 89.87\n",
      "\n",
      "Epoch: 32\n",
      "Train Set Loss: 0.018663700077356772\n",
      "Saving..\n",
      "Test Set Accuracy: 90.1\n",
      "\n",
      "Epoch: 33\n",
      "Train Set Loss: 0.018363898331280798\n",
      "Test Set Accuracy: 89.97\n",
      "\n",
      "Epoch: 34\n",
      "Train Set Loss: 0.018000310727590695\n",
      "Test Set Accuracy: 89.84\n",
      "\n",
      "Epoch: 35\n",
      "Train Set Loss: 0.017998949636593462\n",
      "Test Set Accuracy: 89.97\n",
      "\n",
      "Epoch: 36\n",
      "Train Set Loss: 0.017744745783219114\n",
      "Saving..\n",
      "Test Set Accuracy: 90.29\n",
      "\n",
      "Epoch: 37\n",
      "Train Set Loss: 0.017591079893745483\n",
      "Test Set Accuracy: 90.22\n",
      "\n",
      "Epoch: 38\n",
      "Train Set Loss: 0.017437250522132963\n",
      "Test Set Accuracy: 90.0\n",
      "\n",
      "Epoch: 39\n",
      "Train Set Loss: 0.017216026478298008\n",
      "Test Set Accuracy: 90.24\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 20  \n",
    "\n",
    "for epoch in range(start_epoch, start_epoch+20):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zoRGtBBaRF8g"
   },
   "outputs": [],
   "source": [
    "start_epoch = 40 \n",
    "\n",
    "for epoch in range(start_epoch, start_epoch+60):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q3OJaGSsRGCp"
   },
   "outputs": [],
   "source": [
    "start_epoch = 100\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch+70):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8m0tUNP3QvhZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VkQa44cg2oUo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
