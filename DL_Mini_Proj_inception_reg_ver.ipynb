{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet For CIFAR0-10"
      ],
      "metadata": {
        "id": "5Cgn58bGLA7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/My\\ Drive/DL_Mini_Proj\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzleoGvIa3JV",
        "outputId": "3538b717-fc51-4f8f-a090-33bba3da645f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[Errno 2] No such file or directory: '/content/drive/My Drive/DL_Mini_Proj'\n",
            "/content\n",
            "checkpoint  data  drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "#import renet\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "from torchsummary import summary\n",
        "import torch.utils.data as data"
      ],
      "metadata": {
        "id": "enCe4Ish2n47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Construction"
      ],
      "metadata": {
        "id": "EgqUa4LeK3Tr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InceptionModule(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(InceptionModule, self).__init__()\n",
        "\n",
        "        # Branch 1: 1x1 convolution\n",
        "        self.branch1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Branch 2: 1x1 convolution followed by 3x3 convolution\n",
        "        self.branch2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1_out = self.branch1(x)\n",
        "        branch2_out = self.branch2(x)\n",
        "\n",
        "        return torch.cat([branch1_out, branch2_out], 1)\n",
        "\n",
        "class InceptionResNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(InceptionResNet, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self._make_layer(64, 64, 1)\n",
        "        self.drop1 = nn.Dropout(p=0.1)\n",
        "        self.layer2 = self._make_layer(128, 128, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(256, 256, 2, stride=2)\n",
        "        self.drop2 = nn.Dropout(p=0.25)\n",
        "        self.layer4 = self._make_layer(512, 512, 1, stride=2)\n",
        "        \n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(1024, num_classes)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, num_blocks, stride=1):\n",
        "        layers = []\n",
        "        for _ in range(num_blocks):\n",
        "            layers.append(InceptionModule(in_channels, out_channels))\n",
        "            in_channels = out_channels * 2\n",
        "\n",
        "        if stride == 2:\n",
        "            layers.append(nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.drop1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.drop2(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Initialize the Inception-ResNet model"
      ],
      "metadata": {
        "id": "13PaS06K0N3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = InceptionResNet(num_classes=10).to(device)"
      ],
      "metadata": {
        "id": "JZ5l09322n6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model, (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f99yZ7-kNwN9",
        "outputId": "abe41fa8-cdab-40c4-eaf4-a2748c03bc46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 16, 16]           9,472\n",
            "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
            "              ReLU-3           [-1, 64, 16, 16]               0\n",
            "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
            "            Conv2d-5             [-1, 64, 8, 8]           4,160\n",
            "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
            "              ReLU-7             [-1, 64, 8, 8]               0\n",
            "            Conv2d-8             [-1, 64, 8, 8]           4,160\n",
            "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
            "             ReLU-10             [-1, 64, 8, 8]               0\n",
            "           Conv2d-11             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-12             [-1, 64, 8, 8]             128\n",
            "             ReLU-13             [-1, 64, 8, 8]               0\n",
            "  InceptionModule-14            [-1, 128, 8, 8]               0\n",
            "          Dropout-15            [-1, 128, 8, 8]               0\n",
            "           Conv2d-16            [-1, 128, 8, 8]          16,512\n",
            "      BatchNorm2d-17            [-1, 128, 8, 8]             256\n",
            "             ReLU-18            [-1, 128, 8, 8]               0\n",
            "           Conv2d-19            [-1, 128, 8, 8]          16,512\n",
            "      BatchNorm2d-20            [-1, 128, 8, 8]             256\n",
            "             ReLU-21            [-1, 128, 8, 8]               0\n",
            "           Conv2d-22            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-23            [-1, 128, 8, 8]             256\n",
            "             ReLU-24            [-1, 128, 8, 8]               0\n",
            "  InceptionModule-25            [-1, 256, 8, 8]               0\n",
            "           Conv2d-26            [-1, 128, 8, 8]          32,896\n",
            "      BatchNorm2d-27            [-1, 128, 8, 8]             256\n",
            "             ReLU-28            [-1, 128, 8, 8]               0\n",
            "           Conv2d-29            [-1, 128, 8, 8]          32,896\n",
            "      BatchNorm2d-30            [-1, 128, 8, 8]             256\n",
            "             ReLU-31            [-1, 128, 8, 8]               0\n",
            "           Conv2d-32            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-33            [-1, 128, 8, 8]             256\n",
            "             ReLU-34            [-1, 128, 8, 8]               0\n",
            "  InceptionModule-35            [-1, 256, 8, 8]               0\n",
            "        MaxPool2d-36            [-1, 256, 4, 4]               0\n",
            "           Conv2d-37            [-1, 256, 4, 4]          65,792\n",
            "      BatchNorm2d-38            [-1, 256, 4, 4]             512\n",
            "             ReLU-39            [-1, 256, 4, 4]               0\n",
            "           Conv2d-40            [-1, 256, 4, 4]          65,792\n",
            "      BatchNorm2d-41            [-1, 256, 4, 4]             512\n",
            "             ReLU-42            [-1, 256, 4, 4]               0\n",
            "           Conv2d-43            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-44            [-1, 256, 4, 4]             512\n",
            "             ReLU-45            [-1, 256, 4, 4]               0\n",
            "  InceptionModule-46            [-1, 512, 4, 4]               0\n",
            "           Conv2d-47            [-1, 256, 4, 4]         131,328\n",
            "      BatchNorm2d-48            [-1, 256, 4, 4]             512\n",
            "             ReLU-49            [-1, 256, 4, 4]               0\n",
            "           Conv2d-50            [-1, 256, 4, 4]         131,328\n",
            "      BatchNorm2d-51            [-1, 256, 4, 4]             512\n",
            "             ReLU-52            [-1, 256, 4, 4]               0\n",
            "           Conv2d-53            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-54            [-1, 256, 4, 4]             512\n",
            "             ReLU-55            [-1, 256, 4, 4]               0\n",
            "  InceptionModule-56            [-1, 512, 4, 4]               0\n",
            "        MaxPool2d-57            [-1, 512, 2, 2]               0\n",
            "          Dropout-58            [-1, 512, 2, 2]               0\n",
            "           Conv2d-59            [-1, 512, 2, 2]         262,656\n",
            "      BatchNorm2d-60            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-61            [-1, 512, 2, 2]               0\n",
            "           Conv2d-62            [-1, 512, 2, 2]         262,656\n",
            "      BatchNorm2d-63            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-64            [-1, 512, 2, 2]               0\n",
            "           Conv2d-65            [-1, 512, 2, 2]       2,359,808\n",
            "      BatchNorm2d-66            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-67            [-1, 512, 2, 2]               0\n",
            "  InceptionModule-68           [-1, 1024, 2, 2]               0\n",
            "        MaxPool2d-69           [-1, 1024, 1, 1]               0\n",
            "AdaptiveAvgPool2d-70           [-1, 1024, 1, 1]               0\n",
            "           Linear-71                   [-1, 10]          10,250\n",
            "================================================================\n",
            "Total params: 4,926,666\n",
            "Trainable params: 4,926,666\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 3.13\n",
            "Params size (MB): 18.79\n",
            "Estimated Total Size (MB): 21.93\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construction Complete"
      ],
      "metadata": {
        "id": "P1F2LXoFL5K8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing"
      ],
      "metadata": {
        "id": "QwDtf6ypL-E9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Cutout(object):\n",
        "  \"\"\"Randomly mask out one or more patches from an image.\n",
        "  Args:\n",
        "      n_holes (int): Number of patches to cut out of each image.\n",
        "      length (int): The length (in pixels) of each square patch.\n",
        "  \"\"\"\n",
        "  def __init__(self, n_holes, length):\n",
        "      self.n_holes = n_holes\n",
        "      self.length = length\n",
        "\n",
        "  def __call__(self, img):\n",
        "      \"\"\"\n",
        "      Args:\n",
        "          img (Tensor): Tensor image of size (C, H, W).\n",
        "      Returns:\n",
        "          Tensor: Image with n_holes of dimension length x length cut out of it.\n",
        "      \"\"\"\n",
        "      h = img.size(1)\n",
        "      w = img.size(2)\n",
        "\n",
        "      mask = np.ones((h, w), np.float32)\n",
        "\n",
        "      for n in range(self.n_holes):\n",
        "          y = np.random.randint(h)\n",
        "          x = np.random.randint(w)\n",
        "\n",
        "          y1 = np.clip(y - self.length // 2, 0, h)\n",
        "          y2 = np.clip(y + self.length // 2, 0, h)\n",
        "          x1 = np.clip(x - self.length // 2, 0, w)\n",
        "          x2 = np.clip(x + self.length // 2, 0, w)\n",
        "\n",
        "          mask[y1: y2, x1: x2] = 0.\n",
        "\n",
        "      mask = torch.from_numpy(mask)\n",
        "      mask = mask.expand_as(img)\n",
        "      img = img * mask\n",
        "\n",
        "      return img"
      ],
      "metadata": {
        "id": "TneJZzB8Jmqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_CIFAR10(batch_size, train_ratio):\n",
        "\n",
        "  ROOT = './data'\n",
        "  trainset = torchvision.datasets.CIFAR10(\n",
        "      root = ROOT,\n",
        "      train = True, \n",
        "      download = True\n",
        "  )\n",
        "\n",
        "  # Compute means and standard deviations\n",
        "  means = trainset.data.mean(axis=(0,1,2)) / 255\n",
        "  stds = trainset.data.std(axis=(0,1,2)) / 255\n",
        "  #print(means, stds)\n",
        "\n",
        "  # Preprocess setting\n",
        "  transform_train = transforms.Compose([\n",
        "      transforms.RandomCrop(32, padding=4),\n",
        "      transforms.RandomHorizontalFlip(),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean=means, std=stds),\n",
        "      Cutout(n_holes=1, length=16)\n",
        "  ])\n",
        "  transform_test = transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean=means, std=stds)\n",
        "  ])\n",
        "\n",
        "  # Load the dataset\n",
        "  trainset = torchvision.datasets.CIFAR10(\n",
        "      root = ROOT, \n",
        "      train = True, \n",
        "      download = True, \n",
        "      transform = transform_train\n",
        "  )\n",
        "  testset = torchvision.datasets.CIFAR10(\n",
        "      root = ROOT, \n",
        "      train = False, \n",
        "      download = True, \n",
        "      transform = transform_test\n",
        "  )\n",
        "\n",
        "  train_iterator = data.DataLoader(trainset, batch_size)\n",
        "  test_iterator = data.DataLoader(testset, batch_size)\n",
        "\n",
        "  return train_iterator, test_iterator\n",
        "  \"\"\"\n",
        "  # Split trainset for validset\n",
        "  n_train = int(len(trainset) * train_ratio)\n",
        "  n_valid = len(trainset) - n_train\n",
        "  train_dataset, valid_dataset = data.random_split(trainset, [n_train, n_valid])\n",
        "  \n",
        "  # Build dataloader\n",
        "  train_iterator = data.DataLoader(train_dataset, batch_size)\n",
        "  valid_iterator = data.DataLoader(valid_dataset, batch_size)\n",
        "  test_iterator = data.DataLoader(testset, batch_size)\n",
        "\n",
        "  return train_iterator, valid_iterator, test_iterator\n",
        "  \"\"\""
      ],
      "metadata": {
        "id": "1DTMZC3WJmwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainloader, validloader, testloader = load_CIFAR10(batch_size=16, train_ratio=1)\n",
        "trainloader, testloader = load_CIFAR10(batch_size=16, train_ratio=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21PibWhZJ6da",
        "outputId": "1eec05f0-d230-4423-8b60-75094d2c3d27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "YcJteBmC2n8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "066729ed-3c40-405e-ef0f-a45f00fe1fce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntransform_train = transforms.Compose([\\n    transforms.RandomCrop(32, padding=4),\\n    transforms.RandomHorizontalFlip(),\\n    transforms.ToTensor(),\\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\\n])\\n\\ntransform_test = transforms.Compose([\\n    transforms.ToTensor(),\\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\\n])\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_train) # change transform in future\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test) # change transform in future\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4HFVdimn2n-O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "a077aeac-74fa-4a86-e92c-ae83670d0aec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ntrainset = torchvision.datasets.CIFAR10(\\n    root='./data', train=True, download=True, transform=transform_train) # change transform in future\\ntrainloader = torch.utils.data.DataLoader(\\n    trainset, batch_size=128, shuffle=True, num_workers=2)\\n\\ntestset = torchvision.datasets.CIFAR10(\\n    root='./data', train=False, download=True, transform=transform_test) # change transform in future\\ntestloader = torch.utils.data.DataLoader(\\n    testset, batch_size=100, shuffle=False, num_workers=2)\\n\\nclasses = ('plane', 'car', 'bird', 'cat', 'deer',\\n           'dog', 'frog', 'horse', 'ship', 'truck')\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)#,weight_decay=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
      ],
      "metadata": {
        "id": "v6RvRO3e6Rjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "    print(\"Train Set Loss:\",train_loss/total)\n",
        "\n"
      ],
      "metadata": {
        "id": "JbBSkXWE2n_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(epoch):\n",
        "    global best_acc\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    # Save checkpoint.\n",
        "    acc = 100.*correct/total\n",
        "    if acc > best_acc:\n",
        "        print('Saving..')\n",
        "        state = {\n",
        "            'net': model.state_dict(),\n",
        "            'acc': acc,\n",
        "            'epoch': epoch,\n",
        "        }\n",
        "        if not os.path.isdir('checkpoint'):\n",
        "            os.mkdir('checkpoint')\n",
        "        torch.save(state, './checkpoint/ckpt.pth')\n",
        "        best_acc = acc\n",
        "    print('Test Set Accuracy:',acc)\n",
        "    return acc\n"
      ],
      "metadata": {
        "id": "LxH7SkJJ2oBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
        "\n",
        "for epoch in range(start_epoch, start_epoch+20):\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "id": "pzs360FC2oKB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aa77317-85d2-4309-c6b9-841d5d10216f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 0\n",
            "Train Set Loss: 0.12435709532499313\n",
            "Saving..\n",
            "Test Set Accuracy: 40.17\n",
            "\n",
            "Epoch: 1\n",
            "Train Set Loss: 0.10413713613629341\n",
            "Saving..\n",
            "Test Set Accuracy: 46.68\n",
            "\n",
            "Epoch: 2\n",
            "Train Set Loss: 0.09501234260678292\n",
            "Saving..\n",
            "Test Set Accuracy: 56.04\n",
            "\n",
            "Epoch: 3\n",
            "Train Set Loss: 0.08676004169225693\n",
            "Saving..\n",
            "Test Set Accuracy: 60.75\n",
            "\n",
            "Epoch: 4\n",
            "Train Set Loss: 0.08002455587863923\n",
            "Saving..\n",
            "Test Set Accuracy: 64.44\n",
            "\n",
            "Epoch: 5\n",
            "Train Set Loss: 0.07480791354775429\n",
            "Saving..\n",
            "Test Set Accuracy: 67.1\n",
            "\n",
            "Epoch: 6\n",
            "Train Set Loss: 0.07053399376451969\n",
            "Saving..\n",
            "Test Set Accuracy: 67.18\n",
            "\n",
            "Epoch: 7\n",
            "Train Set Loss: 0.0674082960832119\n",
            "Saving..\n",
            "Test Set Accuracy: 70.38\n",
            "\n",
            "Epoch: 8\n",
            "Train Set Loss: 0.06464856996834278\n",
            "Saving..\n",
            "Test Set Accuracy: 70.74\n",
            "\n",
            "Epoch: 9\n",
            "Train Set Loss: 0.06277236208796501\n",
            "Saving..\n",
            "Test Set Accuracy: 73.16\n",
            "\n",
            "Epoch: 10\n",
            "Train Set Loss: 0.06034281284093857\n",
            "Saving..\n",
            "Test Set Accuracy: 74.38\n",
            "\n",
            "Epoch: 11\n",
            "Train Set Loss: 0.05876245109558106\n",
            "Test Set Accuracy: 73.43\n",
            "\n",
            "Epoch: 12\n",
            "Train Set Loss: 0.057294746780395506\n",
            "Saving..\n",
            "Test Set Accuracy: 75.5\n",
            "\n",
            "Epoch: 13\n",
            "Train Set Loss: 0.0558021182101965\n",
            "Saving..\n",
            "Test Set Accuracy: 76.74\n",
            "\n",
            "Epoch: 14\n",
            "Train Set Loss: 0.05441540223121643\n",
            "Saving..\n",
            "Test Set Accuracy: 77.59\n",
            "\n",
            "Epoch: 15\n",
            "Train Set Loss: 0.05315229023814201\n",
            "Saving..\n",
            "Test Set Accuracy: 78.76\n",
            "\n",
            "Epoch: 16\n",
            "Train Set Loss: 0.05167149598568678\n",
            "Test Set Accuracy: 78.43\n",
            "\n",
            "Epoch: 17\n",
            "Train Set Loss: 0.05096915660917759\n",
            "Test Set Accuracy: 77.36\n",
            "\n",
            "Epoch: 18\n",
            "Train Set Loss: 0.049658230416923764\n",
            "Saving..\n",
            "Test Set Accuracy: 79.8\n",
            "\n",
            "Epoch: 19\n",
            "Train Set Loss: 0.04893088031411171\n",
            "Test Set Accuracy: 79.56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for g in optimizer.param_groups:\n",
        "    print(optimizer.param_groups[0]['lr'])\n",
        "    g['lr'] = 0.001\n",
        "\"\"\"\n",
        "state = {\n",
        "      'net': model.state_dict()\n",
        "}\n",
        "if not os.path.isdir('checkpoint'):\n",
        "  os.mkdir('checkpoint')\n",
        "torch.save(state, './checkpoint/20Epoch.pth')"
      ],
      "metadata": {
        "id": "1XS9hEnaR0M2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001,weight_decay=1e-6)"
      ],
      "metadata": {
        "id": "43wfxq1E1Xy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 20  \n",
        "\n",
        "for epoch in range(start_epoch, start_epoch+20):\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulxHcGSzRFwn",
        "outputId": "825c43ff-1784-4153-819e-ccd7744f93e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 20\n",
            "Train Set Loss: 0.06111984935224056\n",
            "Test Set Accuracy: 76.53\n",
            "\n",
            "Epoch: 21\n",
            "Train Set Loss: 0.056872786796987054\n",
            "Test Set Accuracy: 77.37\n",
            "\n",
            "Epoch: 22\n",
            "Train Set Loss: 0.054467707093656066\n",
            "Test Set Accuracy: 78.47\n",
            "\n",
            "Epoch: 23\n",
            "Train Set Loss: 0.052736565395891666\n",
            "Test Set Accuracy: 79.78\n",
            "\n",
            "Epoch: 24\n",
            "Train Set Loss: 0.05089716591179371\n",
            "Saving..\n",
            "Test Set Accuracy: 79.98\n",
            "\n",
            "Epoch: 25\n",
            "Train Set Loss: 0.04973517975986004\n",
            "Saving..\n",
            "Test Set Accuracy: 80.31\n",
            "\n",
            "Epoch: 26\n",
            "Train Set Loss: 0.048367037560641764\n",
            "Saving..\n",
            "Test Set Accuracy: 80.35\n",
            "\n",
            "Epoch: 27\n",
            "Train Set Loss: 0.047496571877896784\n",
            "Saving..\n",
            "Test Set Accuracy: 81.09\n",
            "\n",
            "Epoch: 28\n",
            "Train Set Loss: 0.04673378049641848\n",
            "Saving..\n",
            "Test Set Accuracy: 81.87\n",
            "\n",
            "Epoch: 29\n",
            "Train Set Loss: 0.04588644198894501\n",
            "Test Set Accuracy: 81.37\n",
            "\n",
            "Epoch: 30\n",
            "Train Set Loss: 0.04482184986442327\n",
            "Test Set Accuracy: 81.79\n",
            "\n",
            "Epoch: 31\n",
            "Train Set Loss: 0.04400566786378622\n",
            "Saving..\n",
            "Test Set Accuracy: 83.04\n",
            "\n",
            "Epoch: 32\n",
            "Train Set Loss: 0.04339637280225754\n",
            "Test Set Accuracy: 83.0\n",
            "\n",
            "Epoch: 33\n",
            "Train Set Loss: 0.04295982111155987\n",
            "Test Set Accuracy: 82.9\n",
            "\n",
            "Epoch: 34\n",
            "Train Set Loss: 0.04221132900506258\n",
            "Saving..\n",
            "Test Set Accuracy: 83.48\n",
            "\n",
            "Epoch: 35\n",
            "Train Set Loss: 0.04187333113223314\n",
            "Saving..\n",
            "Test Set Accuracy: 83.66\n",
            "\n",
            "Epoch: 36\n",
            "Train Set Loss: 0.040972914422899485\n",
            "Test Set Accuracy: 83.16\n",
            "\n",
            "Epoch: 37\n",
            "Train Set Loss: 0.04076682301700115\n",
            "Saving..\n",
            "Test Set Accuracy: 83.74\n",
            "\n",
            "Epoch: 38\n",
            "Train Set Loss: 0.03991217172145844\n",
            "Saving..\n",
            "Test Set Accuracy: 84.29\n",
            "\n",
            "Epoch: 39\n",
            "Train Set Loss: 0.039652029202282425\n",
            "Saving..\n",
            "Test Set Accuracy: 84.73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 40  \n",
        "\n",
        "for epoch in range(start_epoch, start_epoch+20):\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "id": "9AFmB7xGRF6S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84197420-704a-4a33-ec07-119a2cc6e6bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 40\n",
            "Train Set Loss: 0.03933024534359574\n",
            "Test Set Accuracy: 84.0\n",
            "\n",
            "Epoch: 41\n",
            "Train Set Loss: 0.03863885339826346\n",
            "Test Set Accuracy: 83.97\n",
            "\n",
            "Epoch: 42\n",
            "Train Set Loss: 0.038449449110776183\n",
            "Saving..\n",
            "Test Set Accuracy: 84.91\n",
            "\n",
            "Epoch: 43\n",
            "Train Set Loss: 0.03798370545908809\n",
            "Test Set Accuracy: 84.66\n",
            "\n",
            "Epoch: 44\n",
            "Train Set Loss: 0.037390205317288636\n",
            "Test Set Accuracy: 84.39\n",
            "\n",
            "Epoch: 45\n",
            "Train Set Loss: 0.036924358177632094\n",
            "Test Set Accuracy: 84.75\n",
            "\n",
            "Epoch: 46\n",
            "Train Set Loss: 0.036688906094133854\n",
            "Test Set Accuracy: 84.78\n",
            "\n",
            "Epoch: 47\n",
            "Train Set Loss: 0.03652169727616012\n",
            "Saving..\n",
            "Test Set Accuracy: 85.04\n",
            "\n",
            "Epoch: 48\n",
            "Train Set Loss: 0.03618277742251754\n",
            "Saving..\n",
            "Test Set Accuracy: 85.47\n",
            "\n",
            "Epoch: 49\n",
            "Train Set Loss: 0.03565346192106605\n",
            "Test Set Accuracy: 85.18\n",
            "\n",
            "Epoch: 50\n",
            "Train Set Loss: 0.035647349169552325\n",
            "Test Set Accuracy: 85.31\n",
            "\n",
            "Epoch: 51\n",
            "Train Set Loss: 0.03535126780733466\n",
            "Test Set Accuracy: 84.88\n",
            "\n",
            "Epoch: 52\n",
            "Train Set Loss: 0.03477174524277449\n",
            "Saving..\n",
            "Test Set Accuracy: 85.71\n",
            "\n",
            "Epoch: 53\n",
            "Train Set Loss: 0.03441815001085401\n",
            "Test Set Accuracy: 85.27\n",
            "\n",
            "Epoch: 54\n",
            "Train Set Loss: 0.03461835622176528\n",
            "Test Set Accuracy: 85.63\n",
            "\n",
            "Epoch: 55\n",
            "Train Set Loss: 0.03403878188207746\n",
            "Saving..\n",
            "Test Set Accuracy: 85.91\n",
            "\n",
            "Epoch: 56\n",
            "Train Set Loss: 0.03389089255958796\n",
            "Test Set Accuracy: 85.64\n",
            "\n",
            "Epoch: 57\n",
            "Train Set Loss: 0.03371512546852231\n",
            "Test Set Accuracy: 85.84\n",
            "\n",
            "Epoch: 58\n",
            "Train Set Loss: 0.033417165852412584\n",
            "Saving..\n",
            "Test Set Accuracy: 86.12\n",
            "\n",
            "Epoch: 59\n",
            "Train Set Loss: 0.032972260534390806\n",
            "Test Set Accuracy: 86.06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zoRGtBBaRF8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q3OJaGSsRGCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8m0tUNP3QvhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Accuracy: ',test(0))"
      ],
      "metadata": {
        "id": "COW3YCVV2oNa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a31f094-4823-4971-f7df-64c6d6dc6deb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving..\n",
            "Accuracy: 10.0\n",
            "Accuracy:  10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VkQa44cg2oUo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}